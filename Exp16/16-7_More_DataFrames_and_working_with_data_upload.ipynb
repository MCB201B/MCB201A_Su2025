{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b1d07f-d0f3-4635-9c18-e182fc6b8d12",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">16.7 More DataFrames and <br /> working with your data in Python</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 750px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef81ddbb-6f9e-49ac-b527-a0ac44f19f83",
   "metadata": {},
   "source": [
    "Yesterday, we went over some basic functionality of DataFrames, specifically how to retrieve data and how to apply conditional statements to filter DataFrames based on some quantitative value. Today, we'll learn a little bit more about DataFrames, practice a bit more on working with DataFrames, and then begin working with some of the data that you've generated this summer. \n",
    "\n",
    "As you're now getting more familiar with the basics of Python, see if you can challenge yourself to begin writing more and more compact code, reducing the numbr of actions needed to complete a certain task.\n",
    "\n",
    "<strong>Today's learning objectives:</strong>\n",
    "<ol>\n",
    "    <li>Outputting DataFrames</li>\n",
    "    <li>Combining DataFrames</li>\n",
    "    <li>DataFrame NaN values</li>\n",
    "    <li>Sorting DataFrames</li>\n",
    "    <li>DataFrame filtering by qualitative values</li>\n",
    "    <li>Working with our dataset</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12ba2a-4249-4700-88c4-5c525459181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9705ed9-4a36-46bd-9f95-d83d0c1b41e9",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Exercises</h1>\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 400px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf363d-b388-4c63-a6d6-7a0aa9e69c31",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 32px;\">Exercise #1: Outputting DataFrames into notebook</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb49bad-9b59-41bc-b2c3-896f3147078d",
   "metadata": {},
   "source": [
    "As you saw in yesterday's exercises, you could output a DataFrame's contents using either <code>print()</code> or <code>df.style</code>. The DataFrame that we were working with was manageable in size and not too unwieldy. But what happens if we have a large DataFrame consisting of maybe hundreds of rows?\n",
    "\n",
    "For this exercise, take what you've learned so far, and see if you can in a single logical line of code, set up a large 100 rows x 4 columns DataFrame consisting of randomly generated integers in the range [0, 200) with column labels <code>'Tony'</code>, <code>'Victor'</code>, <code>'Liebchen'</code>, and <code>'Barbara'</code> while also assigning your DataFrame to a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7dd5c5-26eb-4a7f-a9f2-f8c52b5d2ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b38325b8-72a3-4eb0-9468-68ce38f697a9",
   "metadata": {},
   "source": [
    "You can either output the DataFrame by calling up the variable, using the <code>print()</code> function, or by using the <code>df.style</code> attribute. Give it a try in the code cell below, and if the output is too large, right-click and select \"Clear cell output\" to remove that output from being displayed in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9902fc-eaf5-43b1-970b-734b617360c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37c517af-bd0d-4e84-83f9-dcd4e3ca6732",
   "metadata": {},
   "source": [
    "How did the output from <code>df.style</code> differ from the other two outputs? \n",
    "\n",
    "You might have noticed that the output from <code>print()</code> and just outputting the variable only shows part of the DataFrame with the middle rows hidden behind ellipses, whereas the stylized table from <code>df.style</code> displays all rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd1a464-2a93-4f5f-8901-c3b144056437",
   "metadata": {},
   "source": [
    "Let's transpose the DataFrame that we created, and then use the <code>print()</code> function to output the transposed DataFrame. See if you can do it in a single logical line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97098461-561a-4eb4-8705-741691fffe45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c325cb0d-e1a4-4d21-886c-45cfc04b65db",
   "metadata": {},
   "source": [
    "What do you notice about the output?\n",
    "\n",
    "You might see that the output also wraps around, and the DataFrame becomes a little less easy to read. Compare that to the <code>df.style</code> output. See if you can also do this in a single logical line, where transpose the original DataFrame and output a stylized table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc5eef-05d7-4e47-ac64-376f9ed1764e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eb7561d-4536-4050-9c7f-7555c943d898",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 32px;\">Exercise #2: DataFrame head and tail attributes</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab785a-6448-4065-8b77-94f896b5ad8d",
   "metadata": {},
   "source": [
    "Pandas DataFrames have certain attributes, such as a head which is calld up by <code>df.head()</code>, and a tail which is called up by <code>df.tail()</code>, which by default correspond to the first five and last five rows of your DataFrame, respectively. \n",
    "\n",
    "So, if you just wanted to do a quick check to see if your data was imported into your notebook correctly, you can make use of the <code>df.head()</code> or <code>df.tail()</code> attributes.\n",
    "\n",
    "Let's use our large DataFrame from exercise #1. Test out those two functions and see how the output looks. What happens if you include the <code>.style</code> attribute after pulling either the head or tail (in the same line)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24b8f3-421c-47af-aab1-82b02d7e1e88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ead3ecb-cbaf-42a2-8350-ddcbf4d241c3",
   "metadata": {},
   "source": [
    "Dig into the documentation for <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html\" rel=\"noopener noreferrer\"><u>the <code>df.head()</code> attribute</u></a> and <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.tail.html\" rel=\"noopener noreferrer\"><u>the <code>df.tail()</code> attribute</u></a>. What will you need to adjust in order to pull the first 10 rows or bottom 10 rows?\n",
    "\n",
    "Give it a try below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af090310-15b2-4fc2-80d8-c60941b2ceab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e65ca62-2884-49df-9f93-a4827a4e4495",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 32px;\">Exercise #3: Combining a DataFrame</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f503936b-5088-4b90-b64a-94c635e14cc3",
   "metadata": {},
   "source": [
    "To set up for this exercise, let's create a new DataFrame that is derived from our larger one, but we will specifically pull out the first ten rows of the first two columns. Then save it to a new variable. See if you can do it in a single logical line making use of both <code>df.iloc[]</code> and slice notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408fd032-cf33-4686-acc3-bf5efc52820e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18cc4175-70c5-4377-ad82-57f9bde9fc8f",
   "metadata": {},
   "source": [
    "Let's check to make sure we got what we were expecting. We generally want to make a habit of checking that our output looks like how we want it to, basically performing QC and verifying what we're working with.\n",
    "\n",
    "What possible approaches can we take to verify our new DataFrame was properly set up? Use your favorite approach below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2267c24d-119f-448e-8798-5fa6f67af132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0ce0d85-dceb-47db-a6e5-7ac12db4b535",
   "metadata": {},
   "source": [
    "Now we're going to pull some more rows and columns from our original large DataFrame. Pull out the first five rows from the last four columns and save it to a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba813a52-4c57-4ab5-ae55-7ed80feb5ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38b5d2d9-3018-46b3-9f73-a11cc011423c",
   "metadata": {},
   "source": [
    "Check to make sure you got what you were looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b0831b-5eef-4553-a431-d16ec0a979d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87ea1fbf-71f4-4b7b-8c0c-65267a18ed57",
   "metadata": {},
   "source": [
    "Now if we wanted to combine these two DataFrames by concatenating them, we can make use of the <code>pd.concat()</code> function, which allows use to combine our two DataFrames together. <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.concat.html\" rel=\"noopener noreferrer\"><u>Documentation for <code>pd.concat()</code> is here.</u></a>\n",
    "\n",
    "To concatenate our two smaller DataFrames, we would set up the following:\n",
    "\n",
    "```\n",
    "concat_df = pd.concat([small_df_1, small_df_2], axis=1)\n",
    "```\n",
    "\n",
    "<strong>Breaking down this code, we get:</strong>\n",
    "<ul>\n",
    "<li><code>concat_df</code> - this is our variable to which we're assigning something</li>\n",
    "    <li><code>=</code> - this is our assignment operator</li>\n",
    "    <li><code>pd.concat()</code> - this is invoking the pandas package and calling up the concat function</li>\n",
    "    <li><code>[small_df_1, small_df_2]</code> - we pass our two DataFrames together in a list object to the function</li>\n",
    "    <li><code>axis=1</code> - We specify that we want to concatenate our columns </li>\n",
    "</ul>\n",
    "\n",
    "Give it a try below to join your two small DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e2eb60-9c9e-44c8-be0e-74107dd67567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cd0508e-fabd-4644-8676-95bc1e2be371",
   "metadata": {},
   "source": [
    "Now take a look at the combined DataFrame. What do you notice about it after the concatenation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f5034a-ec2a-450d-9504-f55d929a74f2",
   "metadata": {},
   "source": [
    "You can probably see that there are values indicated by <code>NaN</code>. These are indicators that tell you that this is an empty cell - nothing exists in it. You can think of this as an empty cell in Excel or Google Sheets as they denote the same thing - nothing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a748b71-2421-4865-9e80-89bfdca9e4fa",
   "metadata": {},
   "source": [
    "Another way to get the same resulting DataFrame is to use the <code>df.join()</code>. Note how the syntax for the function is different than for <code>pd.concat()</code>. <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html\" rel=\"noopener noreferrer\"><u>Documentation for <code>df.join()</code> can be found here.</u></a>\n",
    "\n",
    "The consequence of this that the way to set up this join is different than the <code>pd.concat()</code> function.\n",
    "\n",
    "So the line of code would look like:\n",
    "\n",
    "```\n",
    "joined_df = small_df_1.join(small_df_2)\n",
    "```\n",
    "\n",
    "<strong>Breaking down this code, we get:</strong>\n",
    "\n",
    "<ul>\n",
    "    <li><code>joined_df</code> - this is our variable</li>\n",
    "    <li><code>=</code> - this is our assignment operator</li>\n",
    "    <li><code>small_df_1</code> - we're calling up our first small DataFrame as the 'left' object</li>\n",
    "    <li><code>.join()</code> This is the join function.</li>\n",
    "    <li><code>small_df_2</code> - we're passing our second small DataFrame as the 'right' object</li>\n",
    "</ul>\n",
    "\n",
    "Give it a try below to join your two small DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc4d53-ff70-4fea-9101-507faae926aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d4555ec-3aae-440a-acdb-c8e62c53d5dd",
   "metadata": {},
   "source": [
    "Don't forget to do a quick check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4b1739-7ee5-4e2c-bae0-0668ba074948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a90d4768-95ed-41ad-9639-3d308c66593a",
   "metadata": {},
   "source": [
    "Do you see any differences between the two aproaches we used to combine our two DataFrames?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac452007-caed-4df1-a0d2-771b393cf420",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 32px;\">Exercise #4: Sorting a DataFrame</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8560cbc-cd7b-41ec-8990-1f934a29efc2",
   "metadata": {},
   "source": [
    "Like in Excel and Google Sheets, we can sort our DataFrame to organize how we want our data displayed. Sorting is done through the <code>df.sort_values()</code> function. <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html\" rel=\"noopener noreferrer\"><u>Documentation for <code>df.sort_values</code> can be found here.\n",
    "\n",
    "Take a look at the documentation to see if you can sort your concatenated DataFrame by descending order by the <code>'Barbara'</code> column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2e698a-70ec-4d5c-9248-ff1f7b943896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae7779d6-2d25-4e66-a667-6e1c3e6c6240",
   "metadata": {},
   "source": [
    "Now output <code>concat_df</code> below. How does it look compared to the sorted output above.\n",
    "\n",
    "One thing that you may notice is that the index values are now all jumbled up. Recall that the index is like the key or label for its row, so when the row is reordered, the index will be too. This an important property of DataFrames because it can allow us to clean, filter, merge, concatenate, etc our DataFrames, and each row will still continue to hold its original index value.\n",
    "\n",
    "Depending on how you set up your code, you might see that the <code>concat_df</code> is no longer sorted like the output above. This is because of the parameter <code>inplace</code>. If <code>inplace=False</code>, then the DataFrame being sorted is not overwritten. In order to access this DataFrame for later operations, you'll need to assign it to a variable.\n",
    "\n",
    "However, you can set <code>inplace=True</code>, and this will perform the sort operation \"in place\", meaning that the <code>concat_df</code> DataFrame will be updated to reflect the sort. In this case, you don't need to assign the output to a variable since the operation was done on the DataFrame.\n",
    "\n",
    "You can try switching that parameter between <code>True</code> and <code>False</code> below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e09f58-db73-451e-bc71-1baab15ba04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bb28a14-30f5-4e27-bf4f-a69c842460aa",
   "metadata": {},
   "source": [
    "You might also see that the <code>NaN</code> values are placed at the bottom of the sorted DataFrame. This is because by default, the function will handle 'empty cells' by putting them all last. You can see this is the case in the documentation for <code>df.sort_values()</code>.\n",
    "\n",
    "You can see in the documentation that you can switch between <code>'first'</code> and <code>'last'</code> and that will change how <code>NaN</code> values are handled during sorting.\n",
    "\n",
    "You can try giving that a test below by setting <code>na_position</code> to either <code>'first'</code> or <code>'last'</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd118955-bab7-4403-8b06-8c6e6fe9803a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d2b9df8-0e51-4045-aa09-60fb34839749",
   "metadata": {},
   "source": [
    "You can also sort multiple columns or rows at a time and simultaneously specify the sorting priority based on the order in which the columns appear in the list that you will pass to the <code>df.sort_values()</code> function. Give it a try below, and try sorting based on values in two or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e0b99-d1da-43be-a564-f8f3949d9c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df.sort_values(by=['Barbara', 'Tony'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9596c4-de76-4798-a2b8-bb1e461ca4ba",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 32px;\">Exercise #5: Reorganizing a DataFrame</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b2b62-bae0-4f45-86d9-a7e4b88e5303",
   "metadata": {},
   "source": [
    "Sometimes you might find yourself needing to reorganize the order in which the columns are read, such as if you want a 'Last name' column to appear before a 'First name' column. If you have a small number of columns or column labels with short strings, you can manually input the order of columns to create a new DataFrame that contains the columns ordered how you want it.\n",
    "\n",
    "```\n",
    "concat_df_reorder = concat_df[['Liebchen', 'Tony', 'Barbara', 'Victor']]\n",
    "```\n",
    "\n",
    "<strong>Breaking down this code, we get:</strong>\n",
    "\n",
    "<ul>\n",
    "    <li><code>concat_df_reorder</code> - this is our variable to which we will asisgn our reorganized DataFrame</li>\n",
    "    <li><code>=</code> - this is our assignment operator</li>\n",
    "    <li><code>concat_df</code> - this is the DataFrame that we want to reorganize</li>\n",
    "    <li><code>[ ]</code> - we are telling Python that we want a column or columns defined by a list of elements that we provide</li>\n",
    "    <li><code>['Liebchen', 'Tony', 'Barbara', 'Victor']</code> - this is the list of elements that we are telling Python to pull columns from and in this order (remember lists are ordered)</li>\n",
    "</ul>\n",
    "\n",
    "What we are essentially doing is creating a list <code>&lbrack;'Liebchen', 'Tony', 'Barbara', 'Victor'&rbrack;</code> that contains the order in which we want our columns to be based on the header values. We then use this list to retrieve the columns from the DataFrame <code>concat_df</code> in the order that we indicated in the list, and then assign the resulting DataFrame to our variable <code>concat_df_reorder</code>.\n",
    "\n",
    "Give it a try below to see if you can reorder the columns of your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c64597-9c01-42db-8791-7e598573d75a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cad3bede-00b1-41bb-8aab-80bc1eab7de9",
   "metadata": {},
   "source": [
    "We can also make use of slice notation to inverse the order of our columns.\n",
    "\n",
    "Slice notation has the following syntax:\n",
    "\n",
    "```\n",
    "[ 1st (inclusive) : last (exclusive) : step size]\n",
    "```\n",
    "\n",
    "And if you want to include everything, you can write it out as follows using slice notation:\n",
    "\n",
    "```\n",
    "[ : ] \n",
    "```\n",
    "\n",
    "or \n",
    "\n",
    "```\n",
    "[ : : ]\n",
    "```\n",
    "\n",
    "You can also similarly specify this along two-dimensions.\n",
    "\n",
    "```\n",
    "[ : , : ]\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "[ :: , :: ]\n",
    "```\n",
    "\n",
    "The colons without values just indicates all positions. You can give that a try below combining the index-based locator <code>df.iloc[]</code> and slice notation in 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9772e-f502-4a9b-9f57-f080875eea9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0c1ca9b-ed2c-45ec-b175-434d6092f8a9",
   "metadata": {},
   "source": [
    "To inverse the order, you can provide a negative value for the step size, and with a step size of <code>-1</code>, the order is reversed. Give that a try below to reverse the order of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e19f18-cd62-4580-9729-323cb1cead91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cfbf236-5497-4185-85ba-acedce676e5f",
   "metadata": {},
   "source": [
    "Pandas also has functions that allow us to drop rows and/or columns. The <code>df.drop()</code> function is useful for  removing a single column or row, and the <code>df.dropna()</code> is helpful to remove rows/columns containing NaN from your DataFrame, particularly if it causes some conflict.\n",
    "\n",
    "<a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\" rel=\"noopener noreferrer\"><u>Documentation for <code>df.drop()</code> is here.</u></a>\n",
    "\n",
    "<a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html\" rel=\"noopener noreferrer\"><u>Documentation for <code> df.dropna()</code> is here</u></a>\n",
    "\n",
    "Take a look at the documentation and see if you can drop a row, column, or ones containing NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8b861-aa60-4dec-84a2-dda9df5764c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d21ee390-a080-4ef8-bfa6-0ad4af54727c",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 32px;\">Exercise #6: Filtering based on qualitative value</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cbc593-6e18-4615-94f4-32fdb0f69a89",
   "metadata": {},
   "source": [
    "You learned yesterday how to filter data based on quantitative values, and here you will use the same syntax to then filter based on qualitative values that aren't a numerical object.\n",
    "\n",
    "First, let's put in some qualitative values into our concatenated DataFrame. To simplify the set up, copy the list below and add a new column <code>'Treat'</code> to the <code>concat_df</code> DataFrame.\n",
    "\n",
    "```\n",
    "['tuna', 'salmon', 'ahi', 'tuna', 'salmon', 'ahi', 'tuna', 'salmon', 'ahi', 'tuna']\n",
    "```\n",
    "\n",
    "Recall from previous lessons how you can call up a non-existent column to populate it with the values that you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643bb90e-4a8e-4f45-9c05-7c5b03423c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3457800-8d9b-4d4b-8f6d-adb9de29bd50",
   "metadata": {},
   "source": [
    "As usual, let's do a sanity check to make sure everything worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd6988a-5a71-40c5-9980-81425238a344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d53838f0-9325-4b7c-9184-d487779fe74c",
   "metadata": {},
   "source": [
    "Now let's say we want to work with only the data collected for when cats have had tuna as a treat. The first thing we'll want to do is to set up a conditional statement to check which positions in the <code>'Treat'</code> column match the string <code>'tuna'</code>.\n",
    "\n",
    "How would you set up a conditional statement to check if <u>just</u> the elements in the <code>'Treat'</code> column meets your condition? Give that a try below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223fc7aa-f8db-4580-a8bc-635a69856c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c0d3c06-2cf2-49f2-8b98-d131fd0d8ea9",
   "metadata": {},
   "source": [
    "What you can see is that we have a Series containing results of the element-wise comparison. Recall from yesterday how we filtered our dataset. We will set it our filtering the same way. Go ahead and filter your dataset so we're only working with rows that meet our tuna requirement, and save it to a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80efe36-9e5d-4918-a341-aa90665a9435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab530f31-e530-4ecf-9239-250ab8dd4dc9",
   "metadata": {},
   "source": [
    "Let's perform a sanity check again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1532cf0d-c865-445b-a8ef-5fa3d439d9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "770e6a5d-f737-49e1-a072-848033f81a9c",
   "metadata": {},
   "source": [
    "Pandas also has developed tools for us to filter our DataFrames too. These tools operate similarly to the operators that we've been using before to set up conditional statements, so we can apply them in the same way to filter our DataFrames.\n",
    "\n",
    "The <code>Series.str.contains()</code> can be helpful to pull values containing a short string of relevance, when the complete strings may contain other non-related information. <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.Series.str.contains.html\" rel=\"noopener noreferrer\"><u>Documentation for <code>Series.str.contains()</code> can be found here.</u></a>\n",
    "\n",
    "You might have noticed that it uses a Series object, but we want to filter a DataFrame. Something to note is that when we pull out a column to evaluate, that column's data type is Series instead of DataFrame.\n",
    "\n",
    "You can see that if you try pulling out a column from a DataFrame and check the type with <code>type()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35a02f-b3bd-4cf3-b871-5100a5f5b8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e1db2ae-2920-46a6-94e6-d634b6da9086",
   "metadata": {},
   "source": [
    "Now make use of the <code>Series.str.contains()</code> function to perform an element-wise evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9430cff-5fe8-4148-bcbe-7f1f452df4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d7301bb-23d1-4867-857d-3c357693d46a",
   "metadata": {},
   "source": [
    "How does the output look? What is it's data type now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5263f7-2615-436f-b361-ca46ed7a3a44",
   "metadata": {},
   "source": [
    "Other functions similar to <code>Series.str.contains()</code> are <code>Series.str.startswith()</code> and <code>Series.str.endswith()</code>. Feel free to play around with these functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56419d3b-ff76-46ae-bc4b-929d372a11e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96df9346-e996-4701-841a-b476dc29e995",
   "metadata": {},
   "source": [
    "See if you can then take what you've learned and pull all the rows that correspond to <code>salmon</code> in the <code>'Treat'</code> set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c3fc44-5db7-45c5-abe1-89df6e814d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f11e88c-419a-4737-9004-6de3c051b2c5",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 32px;\">Exercise #7: Importing data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38673dd4-03a5-453e-870f-83a914e0327b",
   "metadata": {},
   "source": [
    "Now we're moving to working with our own data in Python, and for today's exercises, you'll use your group's MTT data, specifically the .csv file. Upload your group's raw MTT data to the <code>data</code> folder. Double click the .csv file in the File Browser to open it up in JupyterLab. You should be able to view it as a normal spreadsheet in a separate window from this notebook. After return to this notebook.\n",
    "\n",
    "To import a spreadsheet, we will use pandas, which will import the file and also convert it into a DataFrame for us. The function <code>pd.read_csv()</code> will be the main one that you use to import data. <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\" rel=\"noopener noreferrer\"><u>Documentation for <code>pd.read_csv()</code> can be found here.</u></a>\n",
    "\n",
    "Although the function itself is named to read .csv files, it has the ability to import essentially any spreadsheet that has some type of character separating the values from each other. The function parses through the file and sets up the DataFrame much like how Excel and Google Sheets can parse through the same file types to set up a spreadsheet. You may encounter .tsv (tab separated values) files as well as files with other kinds of delimiters.\n",
    "\n",
    "Let's start with a basic import and assign the imported data to a variable. When giving your file name, you'll need to make sure that you specify the correct file path to get to your .csv file. Otherwise the Python interpreter won't be able to find the file. You'll also need to keep in mind that the file path that you provide can either start from the home directory or it can start from this notebook's directory. And the file path will need to be a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1bb9d8-b363-4a01-a598-99ac6b8dfa58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66b1bbda-aa66-4204-a267-650c8a17674b",
   "metadata": {},
   "source": [
    "Let's take a look at the resulting DataFrame. How does your DataFrame look?\n",
    "\n",
    "Recall that the .csv files had these large headers which makes it harder to find what we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c9748b-ce49-46ff-8255-add7fd9071be",
   "metadata": {},
   "source": [
    "So two potential ways are to clean the dataset of its useless rows during import or to clean/filter your DataFrame to get just the relevant rows.\n",
    "\n",
    "If you want to try cleaning at the import step, go back into the documentation and see what parameter you'll need to adjust in order to import only the relevant rows and columns.\n",
    "\n",
    "If you want to clean/filter your DataFrame on your own, you can take what we've learned and pull out the relevant rows containing your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b193222b-94a9-4aff-84f6-c04f18738098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82e9570f-c60e-4f43-aa23-bfa7428598da",
   "metadata": {},
   "source": [
    "Let's take a look at how it all imported and got cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e3bd1b-3086-4394-9434-f8e40898154a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9793d7aa-d793-4d8f-b5d1-1d2d35170bf7",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 32px;\">Exercise #8: Data processing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56f006-f53f-45dc-a854-42fb8494178b",
   "metadata": {},
   "source": [
    "You should have a dataset that has been cleaned up and tidied and is ready for you to do some more data processing. Recall that we will need to subtract the mean background absorbance from all wells.\n",
    "\n",
    "So first, see if you can find the mean background absorbance from your MTT assay results. What is the function that we'll need to use? How would you save the mean background absorbance to a variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e625e-8a60-45e0-a78d-cf2374aa1a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58f5a55-962e-4a83-b7e8-3724ee013862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1d45ae5-991d-47d0-a201-de384e324a24",
   "metadata": {},
   "source": [
    "With the mean background saved in a variable, we can perform some math to subtract the mean from all our wells. Recall that we can perform element-wise operations on DataFrames. See if you can subtract the mean background absorbance from your group's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7294105c-6463-4f03-b140-2b5b73889832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbb1f91c-3c80-4633-8aa4-3ad1f040743e",
   "metadata": {},
   "source": [
    "Now find the mean corrected absorbance for your control and edited groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27a8d3c-552f-4267-8379-fb43c223dfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62594a3d-50e1-4d7a-8d24-adc469fae284",
   "metadata": {},
   "source": [
    "To challenge yourself a little bit, how could you reduce what we've done so far in this exercise (#8) into a single line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb6a02-0ae1-4116-9b42-f4e63f0add39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc239694-cadb-48f6-a3f0-b03a9346f43f",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 32px;\">Exercise #9: Renaming DataFrame labels</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1948a6b-8c46-4c69-be7d-793ec539df99",
   "metadata": {},
   "source": [
    "Depending on how you've imported and cleaned your data, you may need to rename your index labels to be more informative. To do this, you will make use of the <code>df.rename()</code> function. <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html\" rel=\"noopener noreferrer\"><u>Documentation on <code>df.rename()</code> can be found here.</u></a>\n",
    "\n",
    "As indicated in the documention, you can specify if you're renaming columns or rows. In our case, we want to change the rows, and to do this, we'll make use of the <code>index</code> parameter. We'll pass it a dictionary, where the keys of the dictionary match to the current labels, and the values of the dictionary are what I want them to be updated to.\n",
    "\n",
    "For example:\n",
    "\n",
    "If my three rows were indexed by numbers, I could set up a dictionary to map the numbers to what I want to call my sample groups then pass it to the <code>index</code> parameter of <code>df.rename()</code>.\n",
    "\n",
    "Don't forget that if you want to apply those changes directly to your DataFrame, you will need to swap <code>inplace=False</code> to <code>inplace=True</code>. Otherwise you will need to assign it to a variable.\n",
    "\n",
    "See if you can rename your sample groups apprpropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9367611f-9a1f-4873-8422-83dbe0fe20f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5cc6a69-5484-4721-bf75-52434ae8c3eb",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 32px;\">Exercise #10: Exporting your cleaned and processed data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8a3d6-3ece-40eb-a995-9802f0724026",
   "metadata": {},
   "source": [
    "Pandas also lets us export a DataFrame into a .csv file, so we can keep a copy of our cleaned and processed data for future refernce if needed.\n",
    "\n",
    "To do this, we'll make use of the <code>df.to_csv()</code> function. <a href=\"https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html\"><u>Documentation for <code>df.to_csv()</code> can be found here.</u></code>\n",
    "\n",
    "We'll provide our Python interpeter with our file name, what we want to have as a delimiter (denoted by the <code>sep</code> parameter), and it will export the .csv file to the same folder that this notebook is in.\n",
    "\n",
    "Go ahead and give that a try below to see if you can export your cleaned and processed MTT data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e505e349-b391-48c2-b528-3f9f053d6e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65f30857-f9a9-4fd3-8cb7-e61953b65432",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Summary</h1>\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 400px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33c136-4ea2-4906-a7bb-ad4b914dd407",
   "metadata": {},
   "source": [
    "Today, you continued to learn more about DataFrames while getting more practice working with them. You also started analyzing your MTT data in Python and learned how to clean your dataset when it's not formatted in a way that Python and pandas can easily interpret. Over the course of Exp 16, you started getting more famililar with the BiologyHub infrastructure, JupyterLab, and Python. Looking towards next semester, we will pick off where we left off and continue working with our data in Python moving towards data visualization and statistical analysis. Then we will dive into more in-depth analyses, learning to set up analysis pipelines, and analyze big datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
